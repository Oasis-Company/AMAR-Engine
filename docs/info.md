amar engine的真实世界捕获有两种方式
用户输入照片或视频，我们生成3维世界（类似于虚幻引擎里的一个卡关），是一个完整的，可交互的世界，而非一个单纯的3D模型
所以每一个AEID对应的是一个很复杂的系统

第一种是gemini告诉amar engine（一下简称AME）世界的构造，物体的摆放位置；








第二种是我们的长期发展目标：

下面我们来详细说明输入部分，用户在AME里上传媒体，AME就会识别（所以或许我们需要一些专业模型，一个专门来感知空间，一个来识别物体，一个来表述，所以或许我们可以集成LLM，或者把一部分的工作转移至云端，未来云端的占比肯定会越来越大，因为我们任务的复杂度会越来越高）








目前任务，我们先做一个极简的UI应用，发布一个github release：

AME 极简 UI 应用 (MVP) 开发计划
我们的目标是先做一个“地基版”，打通“上传 -> 识别 -> 结构化描述”的链路。

1. 核心功能模块 (Current Task)
Media Uploader: 支持用户拖入照片或短视频。

Cloud Perception Bridge:

将媒体数据流传至云端。

集成 Gemini (或其他 VLM) 进行初级空间感知：识别场景中有哪些物体（Table, Cup, Chair）及其大致相对位置。

System Generator:

接收云端返回的结构化数据。

根据 Skills 规范 自动为识别出的物体分配 Metaclass（例如：识别到“杯子”，自动标记其为 Metaclass: Container）。

AEID Registry: 本地生成一个唯一的 AEID，并将这个微型“系统”的配置保存在本地。

2. 极简 UI 界面设计
风格: 极简主义，类似编译器或纯粹的生产力工具。

布局:

左侧：媒体预览窗口（显示用户上传的照片/视频）。

右侧：实时生成的场景结构树（Scene Graph），显示 AEID、物体列表和对应的元类属性。

底部：状态栏，显示云端模型感知的逻辑进度。

